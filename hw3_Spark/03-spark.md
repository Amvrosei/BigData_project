# Домашнее задание по Spark

* Deadline: 29.09.2024, 23:59

Задачи 1 и 2 можно выполнять как на RDD, так и на DF API. 

Код задач заливать аналогично предыдущим домашкам, в репозиторий `sberspark`.

## Задача 1

#### Исходные данные

* `/data/wiki/en_articles_part` - статьи Википедии. Засылать на тестирование нужно на **частичном датасете** (`en_articles_part` чтоб не перегружать кластер). 


Формат данных:
```
article ID <tab> article text
```

#### Условие
Найдите все пары двух последовательных слов (биграмм), где первое слово «narodnaya». Для каждой пары подсчитайте количество вхождений в тексте статей Википедии. Выведите все пары с их частотой вхождений в лексикографическом порядке. Формат вывода - word_pair <tab> count.

**Пример результата**:
```
narodnaya_station 100500
narodnaya_street 42
```
Обратите внимание, что два слова в паре объединяются с символом нижнего подчеркивания, а результат - в нижнем регистре. В датасете слово narodnaya может встречаться с большой буквы, поэтому сначала приведите всё к нижнему регистру.

#### Техническая информация
При парсинге отбрасывайте все символы, которые не являются латинскими буквами:
```
text = re.sub("^\W+|\W+$", "", text)
```

## Задача 2

#### Исходные данные
* Полный датасет: `/data/twitter/twitter_sample.txt` (при коммите в систему указывайте в коде этот датасет)
* Частичная выборка: `/data/twitter/twitter_sample_small.txt`
**Формат данных:**
```
follower_id \t user_id
```
#### Условие
Дан ориентированный граф.Необходимо найти длину кратчайшего пути между  вершинами 12 и 34 графа, реализовав алгоритм "Поиск в ширину". Если кратчайших путей несколько, выведите первый.
Обратите внимание на критерий остановки алгоритма. В рамках оптимизации вы можете остановить программу раньше, чем закончится поиск в ширину т.к. нам достаточно одно пути. 
Выходной формат: последовательность вершин (учитывая начало и конец), разделенных запятой, без пробелов. Например, путь «12 -> 42 -> 34» должен быть напечатан как: 12,42,34.

#### Дополнительные комментарии
Данных немного поэтому есть соблазн на каком-нибудь этапе решения сделать `take()` или `collect()`, сконвертировав RDD в обычный Python-объект. Конечно, с точки зрения API, работать с обычными объектами привычнее. Но т.к. обычные объекты из коробки не отвечают требованиям высокодоступности и распределённости, такое решение учитываться не будет.
Помните, что по возможности необходимо избегать написания UDF, вместо этого внимательно изучите возможности pyspark.sql.functions. Вам точно пригодится этот модуль.

#### Стартовый фрагмент кода
От этого фрагмента кода можно отталкиваться при решении задачи. Этот код не эффективный поэтому он не будет работать в системе проверки. Его цель - дать понимание, от чего отталкиваться в задаче.
```python
def parse_edge(s):
  user, follower = s.split("\t")
  return (int(user), int(follower))

def step(item):
  prev_v, prev_d, next_v = item[0], item[1][0], item[1][1]
  return (next_v, prev_d + 1)

def complete(item):
  v, old_d, new_d = item[0], item[1][0], item[1][1]
  return (v, old_d if old_d is not None else new_d)

n = 400  # number of partitions
edges = sc.textFile("/data/twitter/twitter_sample_small.txt").map(parse_edge).cache()
forward_edges = edges.map(lambda e: (e[1], e[0])).partitionBy(n).persist()

x = 12
d = 0
distances = sc.parallelize([(x, d)]).partitionBy(n)
while True:
  candidates = distances.join(forward_edges, n).map(step)
  new_distances = distances.fullOuterJoin(candidates, n).map(complete, True).persist()
  count = new_distances.filter(lambda i: i[1] == d + 1).count()
  if count > 0:
    d += 1
    distances = new_distances
  else:
    break
```
Код для создания SparkContext
```python
from pyspark import SparkContext, SparkConf

config = SparkConf().setAppName("my_super_app").setMaster("local[3]")  # конфиг, в котором указываем название приложения и режим выполнения (local[*] для локального запуска, yarn для запуска через YARN).
sc = SparkContext(conf=config)  # создаём контекст, пользуясь конфигом
```
### Дополнительные комментарии
1. Данных немного поэтому есть соблазн на каком-нибудь этапе решения сделать `take()` или `collect()`, сконвертировав RDD / DF в обычный Python-объект. Конечно, с точки зрения API, работать с обычными объектами привычнее. Но т.к. обычные объекты из коробки не отвечают требованиям высокодоступности и распределённости, такое решение учитываться не будет.
2. Помните, что по возможности необходимо избегать написания UDF, вместо этого внимательно изучите возможности `pyspark.sql.functions`. Вам точно пригодится этот модуль.
Для сдачи задания нужно закоммитить в Git PySpark-код, а в run.sh прописать команду для его запуска (`spark-submit <my_code.py>`). 
3. В отличие от ДЗ по MapReduce, логи Spark перенаправлять в /dev/null не нужно.
